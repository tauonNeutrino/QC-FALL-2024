{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dimod\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from dwave.system import LeapHybridSampler, DWaveSampler, EmbeddingComposite\n",
    "# import dwavebinarycsp\n",
    "import dwave.inspector\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.cluster import rand_score, adjusted_rand_score\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "import mplcyberpunk\n",
    "from matplotlib import colors\n",
    "import colorsys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize global variables (to be re-valued in demo() )\n",
    "nv = 1 \n",
    "nt = 5\n",
    "\n",
    "def get_max_coeff(mydict):\n",
    "    return max([abs(v) for v in mydict.values()])\n",
    "\n",
    "def angle_diff(a, b):\n",
    "    return 2*abs((a - b + 0.5) % 1.0 - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This is the part of the code that deals with quantum annealing. Paper reference: https://arxiv.org/abs/1903.08879\n",
    "\n",
    "The first function, g_from_AJpaper_QUBO(), shows the many different\n",
    "ways we can determine this g() factor from the paper. In the paper, it was defined as 1 - e^-mx, which we found to be inefficient \n",
    "last year. We chose 1 + ln(mx) instead, which gave better results\n",
    "\n",
    "create_qubo() creates the problem and translates it into a qubo, which is the mathematical form of the problem that a quantum annealer\n",
    "can compute. This function is based around the paper above, equation 4, which encodes the clustering problem into a qubo.\n",
    "    Remember: the \"qubo\" that we create is a problem based around the ising spin equation, shown in equation 1. \n",
    "    p_ik represents the probability (0, no, or 1, yes) of particle i belonging to vertex k, while p_jk is the same for particle j\n",
    "    The triple summation of equation 4 represents the qubit couplings, where we scale whether we think particles i and j belong\n",
    "        to the same vertex k by using function g(Dij).\n",
    "    The next double summation is a penalty sum, focused on the qubit biases. This means that it enforces the rule that each logical \n",
    "        qubit (each particle) only belongs to one vertex\n",
    "\n",
    "run_qa() receives that completed qubo from create_qubo() and sends it to the annealer itself, along with the parameters num_reads,\n",
    "    chain_strength and annealing_time. These are important later, but here are some summaries:\n",
    "        num_reads: more reads corresponds to your best solution being more accurate (more samples),\n",
    "        chain_strength: this is a very particular value. If it is too high, it will add an uneven bias to pollute our answer,\n",
    "            but if it is too low, then it will not enforce qubit connections that well, which is required to correctly \n",
    "            represent the qubo on the annealing machine\n",
    "        annealing_time: any shorter than 50 microseconds will give bad results because it is too short of a computation to allow bits\n",
    "            to quantum tunnel, and too long is wasteful over high numbers of reads\n",
    "\n",
    "Finally, set_solution_from_annealer_response() receives the finished computations from the annealer (response is a bunch of \n",
    "    dictionaries corresponding to each 'read' from num_reads). These responses are sorted, with the first being the lowest energy,\n",
    "    and therefore the most correct (assuming your QUBO represents the problem). Remember, the physics behind the problem hamiltonians\n",
    "    and qubos that are derived from them is that lower system energies are preferred. This is why we anneal!\n",
    "\n",
    "    After this, the function checks the solutions to \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def g_from_AJpaper_QUBO(m, Dij):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to scale the energy levels\n",
    "    so that the lowest levels are more separated from each other.\n",
    "\n",
    "    This represents the function g(Dij) in the paper, known as the distortion function. It distorts the distance between any points applied to it\n",
    "\n",
    "    This is helpful because we may get better results if we use log(Dij) instead of the function used by the paper\n",
    "    \"\"\"\n",
    "    # Dij -= 2\n",
    "    # Dij *= 5\n",
    "    # print(\"Dij =\", Dij)\n",
    "    # return math.log(Dij)\n",
    "    # return Dij\n",
    "    return 1 + math.log(Dij*m)\n",
    "    # return Dij ** 0.25 + Dij ** 0.5\n",
    "    # return m + math.log(Dij)\n",
    "    # m = 5\n",
    "    # return 1 - math.exp(-m*Dij)\n",
    "\n",
    "def create_qubo(df, m, nv):\n",
    "    Z = df['z']\n",
    "    T = df['theta']\n",
    "    P = df['momentum']\n",
    "    # # # # # #\n",
    "    # # # # # #\n",
    "    nT = len(Z)\n",
    "\n",
    "    qubo = defaultdict(float)\n",
    "    Dij_max = 0\n",
    "\n",
    "    # Define QUBO terms for the first summation\n",
    "    for k in range(nv):\n",
    "        for i in range(nT):\n",
    "            for j in range(i+1, nT):\n",
    "                Dij = ((Z[i] - Z[j])**2 + angle_diff(T[i], T[j])**2) ** 0.5\n",
    "                Dij_max = max(Dij_max, Dij)\n",
    "                # print(g_from_AJpaper_QUBO(m, Dij) + min(P[i], P[j]))\n",
    "                # print(g(m, Dij), min(P[i], P[j]))\n",
    "                # we add the min(P[i], P[j]) term to prevent high momentum tracks from being assigned to the same vertex\n",
    "                qubo[(i+nT*k, j+nT*k)] = g_from_AJpaper_QUBO(m, Dij) + min(P[i], P[j]) # * (P[i] + P[j]) # prevent high momentum tracks from being assigned to same vertex\n",
    "\n",
    "    # print(\"Dij_max\", Dij_max, \"Max before constraint\", get_max_coeff(qubo))\n",
    "    lam = 1.0 * get_max_coeff(qubo)\n",
    "\n",
    "    # Define QUBO terms for penalty summation\n",
    "    # Note, we ignore a constant 1 as it does not affect the optimization\n",
    "    # https://docs.dwavesys.com/docs/latest/c_gs_6.html\n",
    "    for i in range(nT):\n",
    "        for k in range(nv):\n",
    "            qubo[(i+nT*k, i+nT*k)] -= lam\n",
    "            for l in range(k+1, nv):\n",
    "                qubo[(i+nT*k, i+nT*l)] += 2 * lam\n",
    "\n",
    "    return qubo\n",
    "\n",
    "def run_qa(df, qubo):\n",
    "    # # # # \n",
    "    # # # # \n",
    "    strength = math.ceil(get_max_coeff(qubo))\n",
    "    sampler = EmbeddingComposite(DWaveSampler(token=os.getenv(\"TOKEN\")))\n",
    "    response = sampler.sample_qubo(qubo, num_reads=100, chain_strength=strength, annealing_time = 50) # Don't mess with this until you understand it\n",
    "    best = response.first.sample\n",
    "    # TO INSPECT PROBLEM\n",
    "    dwave.inspector.show(response)\n",
    "    # print(best)\n",
    "    set_solution_from_annealer_response(df, best)\n",
    "    \n",
    "def set_solution_from_annealer_response(df, response):\n",
    "    nT = len(df)\n",
    "    track_to_vertex = [None] * nT\n",
    "    # nV = get_nv_from_p(df['momentum'])\n",
    "\n",
    "    for num, bool in response.items(): # this part is important to understand, maybe try printing out the response to understand the definition of i,k\n",
    "        print(response)\n",
    "        if bool == 1:\n",
    "            i = num % nT # track number\n",
    "            k = num // nT # vertex number\n",
    "\n",
    "            if track_to_vertex[i] != None:\n",
    "                print(\"Invalid solution! Track assigned to multiple vertices.\")\n",
    "                # track_to_vertex = None\n",
    "                df['qagroup'] = None\n",
    "                return\n",
    "            else:\n",
    "                track_to_vertex[i] = k\n",
    "    \n",
    "    if None in track_to_vertex:\n",
    "        print(\"Invalid solution! Track assigned to no vertex :(\")\n",
    "        track_to_vertex = None\n",
    "    \n",
    "    else:\n",
    "        print(\"Anneal returned valid results! }:D\")\n",
    "    df['qagroup'] = track_to_vertex\n",
    "    print(\"Printing track_to_vertex, the assigned vertexes to each particle\")\n",
    "    print(track_to_vertex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reasonable_sizes_for_plotting_momentum(P):\n",
    "    return [500 * p**0.6 for p in P]\n",
    "    # return [50 * p ** 0.25 for p in P]\n",
    "\n",
    "def rand_cmap_for_plotting(nlabels, type='bright'):\n",
    "\n",
    "    # Generate color map for bright colors, based on hsv\n",
    "    if type == 'bright':\n",
    "        randHSVcolors = [((np.random.uniform(low=0.0, high=1)),\n",
    "                          np.random.uniform(low=0.7, high=1),\n",
    "                          np.random.uniform(low=0.9, high=1)) for _ in range(nlabels)]\n",
    "\n",
    "        # Convert HSV list to RGB\n",
    "        randRGBcolors = []\n",
    "        for HSVcolor in randHSVcolors:\n",
    "            randRGBcolors.append(colorsys.hsv_to_rgb(HSVcolor[0], HSVcolor[1], HSVcolor[2]))\n",
    "\n",
    "        # random_colormap = LinearSegmentedColormap.from_list('new_map', randRGBcolors, N=nlabels)\n",
    "        return randRGBcolors\n",
    "    # Generate soft pastel colors\n",
    "    if type == 'soft':\n",
    "        low = 0.6\n",
    "        high = 0.95\n",
    "        randRGBcolors = [(np.random.uniform(low=low, high=high),\n",
    "                          np.random.uniform(low=low, high=high),\n",
    "                          np.random.uniform(low=low, high=high)) for _ in range(nlabels)]\n",
    "        # random_colormap = LinearSegmentedColormap.from_list('new_map', randRGBcolors, N=nlabels)\n",
    "        return randRGBcolors\n",
    "\n",
    "\n",
    "palette = rand_cmap_for_plotting(30, type='bright')\n",
    "\n",
    "\n",
    "def plot_clusters(df, grouping, title, saveto=None):\n",
    "    all_zs = df['z']\n",
    "    all_thetas = df['theta']\n",
    "    all_ps = df['momentum']\n",
    "\n",
    "    # palette = ['b', 'g', 'r', 'c', 'm', 'y'] # 6 is enough\n",
    "    colors = [palette[i] for i in grouping]\n",
    "    scaled = get_reasonable_sizes_for_plotting_momentum(all_ps)\n",
    "    # plt.scatter(all_zs, all_thetas, c=all_ps)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Z (normalized)\")\n",
    "    plt.ylabel(\"θ (normalized)\")\n",
    "    plt.grid()\n",
    "    for (i, z) in enumerate(all_zs):\n",
    "        plt.scatter(z, all_thetas[i], color=colors[i], s=scaled[i], alpha=0.5)\n",
    "        # mplcyberpunk.make_scatter_glow()\n",
    "    if saveto is not None:\n",
    "        plt.savefig(saveto)\n",
    "        plt.close() # avoid displaying the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_clusters(truth, solution):\n",
    "    if len(solution) == 0:\n",
    "        return 0.0\n",
    "    if solution.isnull().any():\n",
    "        return 0.0\n",
    "    return 100.0 * adjusted_rand_score(truth, solution)\n",
    "\n",
    "def get_rand_from_number_of_vertices(v):\n",
    "    elem = v[np.random.randint(len(v))]\n",
    "    v.remove(elem)\n",
    "    return elem\n",
    "\n",
    "def generate_clusters(nt=16, nv=2, std=0.02):\n",
    "\n",
    "    z_range = 1 # 0 to 1\n",
    "    theta_range = 1 # 0 to 1 (we're acting like we squished the ranges)\n",
    "    num_tracks = nt\n",
    "\n",
    "    p = np.random.rand(num_tracks)\n",
    "    p = 1/(p**2 + 0.001) # 3 orders of magnitude diff between min and max. corresponds to: 30 MeV, 30 GeV scaled\n",
    "    p /= p.max()\n",
    "\n",
    "    p = np.sort(p)[::-1].tolist()\n",
    "\n",
    "    hard_ps = p[:nv]\n",
    "    soft_ps = p[nv:]\n",
    "\n",
    "    vertex_zs = np.random.rand(nv) * z_range\n",
    "    vertex_thetas = np.random.rand(nv) * theta_range\n",
    "\n",
    "    points_per_cluster = num_tracks // nv\n",
    "    remainder = num_tracks % nv\n",
    "\n",
    "    all_zs = []\n",
    "    all_thetas = []\n",
    "    all_ps = []\n",
    "    truth = [] # which cluster each point belongs to\n",
    "    for i in range(nv):\n",
    "        z = vertex_zs[i]\n",
    "        theta = vertex_thetas[i]\n",
    "\n",
    "        # -1 because we're going to add the vertex itself too\n",
    "        toadd = points_per_cluster - 1 + (1 if i < remainder else 0)\n",
    "        for j in range(toadd):\n",
    "            all_zs.append(z + np.random.normal(scale=std))\n",
    "            all_thetas.append((theta + np.random.normal(scale=std)) % 1.0) # modulo 1.0 to keep it in the range\n",
    "            all_ps.append(get_rand_from_number_of_vertices(soft_ps))\n",
    "            truth.append(i)\n",
    "        all_zs.append(z)\n",
    "        all_thetas.append(theta)\n",
    "        all_ps.append(get_rand_from_number_of_vertices(hard_ps))\n",
    "        truth.append(i)\n",
    "    # all_zs = np.array(all_zs)\n",
    "    # all_thetas = np.array(all_thetas)\n",
    "    DF = pd.DataFrame({'z': all_zs, 'theta': all_thetas, 'momentum': all_ps, 'truegroup': truth})\n",
    "    DF.to_csv('TEST_DATA.csv')\n",
    "    return DF\n",
    "\n",
    "    # return (n, all_zs, all_thetas, all_ps, truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THIS IS THE CENTRALIZED PLACE TO CHANGE PARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "nv = 3\n",
    "nt = 10\n",
    "\n",
    "def demo(): # Simplified to not deal with anti-kt yet. Can change nv, nT, cluster size std\n",
    "    std = 0.06\n",
    "    df = generate_clusters(nt, nv, std=std)\n",
    "    qubo = create_qubo(df, nv-1, nv)\n",
    "    # df = generate_clusters(nt=16, nv=4, std=std)\n",
    "    # df = generate_clusters(nt=12, nv=5, std=std)\n",
    "    plot_clusters(df, df['truegroup'], \"Generated Clusters\")\n",
    "\n",
    "    print(\"DATA GENNED\")\n",
    "    print(df)\n",
    "\n",
    "    print(\"SENDING TO QA\")\n",
    "\n",
    "    run_qa(df, qubo)\n",
    "\n",
    "    print(\"SOLUTION RECEIVED\")\n",
    "    print(df)\n",
    "    qascore = score_clusters(df['truegroup'], df['qagroup'])\n",
    "    \n",
    "    plot_clusters(df, df['qagroup'], f\"Annealer solution, Adj. Rand index {qascore:.1f}%\")\n",
    "    \n",
    "    # print(\"Score:\", score)\n",
    "    print(df)\n",
    "\n",
    "    df['qascore'] = qascore\n",
    "\n",
    "    df.to_csv(\"df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we run the code\n",
    "\n",
    "demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
